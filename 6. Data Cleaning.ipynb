{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning With Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources of Missing Values\n",
    "\n",
    "Standard Missing Values\n",
    "\n",
    "Summarizing Missing Values\n",
    "\n",
    "Analyzing Obesity in England\n",
    "\n",
    "Time Series\n",
    "\n",
    "Basic Time Series Manipulation\n",
    "\n",
    "Time Series on Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumber dari data yang hilang : \n",
    "\n",
    "    Pengguna lupa mengisi kolom.\n",
    "    Data hilang saat mentransfer secara manual dari database lama.\n",
    "    Terjadi kesalahan pemrograman.\n",
    "    Pengguna memilih untuk tidak mengisi bidang yang terkait dengan keyakinan mereka \n",
    "    tentang bagaimana hasil akan digunakan atau ditafsirkan.\n",
    "\n",
    "kalian mulai membersihkan kumpulan data, ada baiknya kalian memahami data secara umum. Setelah itu, kalian bisa menyusun rencana untuk membersihkan data.\n",
    "\n",
    "Mulailah dengan menanyakan pertanyaan-pertanyaan berikut:\n",
    "\n",
    "     Apa saja fiturnya?\n",
    "     Apa tipe yang diharapkan (int, float, string, boolean)?\n",
    "     Apakah ada data yang hilang (nilai yang dapat dideteksi Panda)?\n",
    "     Apakah ada jenis data lain yang hilang yang tidak begitu jelas (tidak dapat dengan mudah dideteksi dengan Pandas)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def downloadDataSet(url,destNameFile): \n",
    "    \"\"\"\n",
    "    Download Data Set\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: \n",
    "        with open(destNameFile, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        response.close()\n",
    "        return True \n",
    "    \n",
    "    response.close()\n",
    "    print(\"Request Error , http statuscode : \",response.status_code)\n",
    "    return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ardhiraka/PFDS_sources/master/property_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namaFile = \"property_data.csv\"\n",
    "status = downloadDataSet(url, namaFile)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(namaFile)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitur-fitur pada kolom adalah:\n",
    "* ST_NUM: Street number\n",
    "* ST_NAME: Street name\n",
    "* OWN_OCCUPIED: Pemiliknya menempati ? \n",
    "* NUM_BEDROOMS: Number of bedrooms\n",
    "* NUM_BATH : number bathrooms \n",
    "\n",
    "SQ_FT : square foot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipe yang diharapkan adalah sebagai berikut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ST_NUM: float or int… some sort of numeric type\n",
    "* ST_NAME: string\n",
    "* OWN_OCCUPIED: string… Y (“Yes”) or N (“No”)\n",
    "* NUM_BEDROOMS: float or int, a numeric type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard missing values adalah missing values yang dapat di deteksi Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### penggunaan apply dan loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.DataFrame([[2,3,1],\n",
    "                    [3,2,2],\n",
    "                    [2,4,4]], columns=list('ABC'))\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.apply(lambda x : x['A'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.apply(lambda x : x['A'], axis = 0) # Error KeyError: 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.apply(lambda x : x, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.iloc[0:1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access a group of rows and columns by label(s) or a boolean array \n",
    "dfa.loc[0]   # default axis = 0 .ndim  1 dim ,   shape 3,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.loc[0:1] # 2 dimensi 3, . m:n n is include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kita akan memberikan background warna merah pada data frame varibale dfa\n",
    "# jika data tersebut harus lebih besar dari baris 0 [2, 3, 1]\n",
    "# try change axis \n",
    "# 2 >  [2,3,1]\n",
    "# 3 > [2,3,1]\n",
    "# 1 > \n",
    "# axis = 0 kolom \n",
    "# axis = 1 baris\n",
    "dfa.style.apply(lambda x: [\"background: red\" if v >= x.iloc[0] else \"\" for v in x], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.DataFrame([[4,9]] * 3, columns=['A', 'B'])\n",
    "dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.apply(np.sum, axis=0)  # apply untuk tipa kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.apply(np.sum, axis=1)  # apply untuk tiap baris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning a list-like will result in a Series [1.2]\n",
    "dfx.apply(lambda x: [1, 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing ``result_type='expand'`` will expand list-like results to columns of a Dataframe\n",
    "dfx.apply(lambda x: [1, 2], axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_num = df.loc[:,['ST_NUM']]\n",
    "street_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def highlight_column_nan(column):    \n",
    "    \"\"\"\n",
    "    Function untuk me return background-color\n",
    "    \"\"\"\n",
    "    highlight = 'background-color: palegreen;'\n",
    "    default = '' \n",
    "    #return [highlight if str(v) == str(1e400*0)  else default for v in column]\n",
    "    return [highlight if math.isnan(v)  else default for v in column]\n",
    "    \n",
    "#df.style.apply(highlight_column_nan, subset=['ST_NUM'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkan beberapa kolom\n",
    "street_num = df.loc[:,['ST_NUM','ST_NAME','NUM_BEDROOMS','OWN_OCCUPIED']]\n",
    "street_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.apply?\n",
    "- 0 or 'index': apply function to each column\n",
    "- 1 or 'columns': apply function to each row\n",
    " sedikit berbeda dengan dataframe axis = 0 baris, axis = 1 kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan apply function highlight_column_nan\n",
    "street_num.style.apply(highlight_column_nan, subset=['ST_NUM'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkan st_num\n",
    "df['ST_NUM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan pengecekan is null/isna\n",
    "df['ST_NUM'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ST_NUM'].isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Standard Missing Values\n",
    "kita coba melihat ada beberapa data yang tidak standar yang akan menjadikan missing values pada num_bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "missing_values = [\"n/a\", \"na\", \"--\"]\n",
    "\n",
    "#str(1e400*0) == NaN\n",
    "def highlight_column_missing_values(column):    \n",
    "    highlight = 'background-color: yellow;'\n",
    "    default = '' \n",
    "    #return [highlight if str(v) == str(1e400*0)  else default for v in column]\n",
    "    return [highlight if str(v) == str(1e400*0) or v in missing_values else default for v in column]\n",
    "    \n",
    "#df.style.apply(highlight_column_nan, subset=['ST_NUM'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_num.style.apply(highlight_column_missing_values, subset=['NUM_BEDROOMS'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di kolom ini ada empaat missing values:\n",
    "- n/a\n",
    "- NA\n",
    "- __\n",
    "- na dari bagian sebelumnya, kita tahu bahwa pandas akan mengenali \"NA\" sebagai missing value, tapi bagaimana dengan yang lain? Mari lihat, dengan pengecekan is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NUM_BEDROOMS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kita coba lakukan pengecekan menggunakan is null\n",
    "df['NUM_BEDROOMS'].isnull()\n",
    "# perhatikan baris 7 dan 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada index 7 dan 8 is null tidak true dengan kata lian tidak di identifikasikan sebagai NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat list missing values\n",
    "missing_values = [\"n/a\", \"na\", \"--\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(namaFile, na_values= missing_values) # di definisakn debgai kategory NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NUM_BEDROOMS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NUM_BEDROOMS'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kali ini, semua format berbeda dikenai sebagai missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unexpected Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sejauh ini kita telah melihat missing values, dan non-standard missing values. Bagaimana jika kita memiliki tipe yang tidak terduga? Misalnya, jika fitur kita diharapkan berupa string, tetapi malah jenisnya numerik, maka secara teknis ini juga merupakan missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita lihat kolom \"Owner Occupied\" untuk mengetahuinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_real = [\"Y\", \"N\", \"y\", \"n\"]\n",
    "\n",
    "def highlight_column_creteria_values(column):\n",
    "    highlight = 'background-color: yellow';\n",
    "    default = ''\n",
    "    #return [highlight if str(v) == str(1e400*0)  else default for v in column]\n",
    "    return [default if v in values_real else highlight for v in column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_num.style.apply(highlight_column_creteria_values, subset=['OWN_OCCUPIED'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OWN_OCCUPIED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OWN_OCCUPIED'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di baris keempat, ada angka 12. Respons untuk Owner Occupied jelas harus berupa string (Y atau N), jadi tipe numerik ini berupa missing value.\n",
    "\n",
    "Contoh ini sedikit lebih rumit sehingga kita perlu memikirkan strategi untuk mendeteksi jenis nilai yang hilang ini. Ada sejumlah pendekatan berbeda, tetapi inilah cara kita akan mengatasinya.\n",
    "\n",
    "* Loop pada kolom OWN_OCCUPIED\n",
    "* Kita coba ubah semua entry menjadi integer\n",
    "* Jika entry tidak dapat diubah menjadi integer, tandai sebagai missing value\n",
    "* Jika tidak dapat menjadi integer, maka kita tau kalau entry adalah string, keep going\n",
    "\n",
    "Mari kita lihat kodenya dan kemudian kita akan membahasnya secara mendetail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for row in df['OWN_OCCUPIED']:\n",
    "    try:\n",
    "        int(row)\n",
    "        df.loc[cnt, 'OWN_OCCUPIED']=np.nan\n",
    "    except ValueError:\n",
    "        pass\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam kode diatas, kita looping setiap entri di kolom \"Owner Occupied\". Untuk mencoba dan mengubah entri menjadi integer, kita menggunakan int(row).\n",
    "\n",
    "Jika nilai dapat diubah menjadi bilangan bulat, kita mengubah entri menjadi missing value menggunakan np.nan Numpy.\n",
    "\n",
    "Di sisi lain, jika tidak dapat diubah menjadi bilangan bulat, kita pass dan keep going.\n",
    "\n",
    "Kalian akan melihat bahwa kita menggunakan try dan except ValueError. Ini disebut exception handling, dan kita menggunakan ini untuk menangani errors.\n",
    "\n",
    "Jika kita mencoba dan mengubah entri menjadi integer dan tidak dapat diubah, ValueError akan di return, dan kode akan berhenti. Untuk mengatasi ini, kita menggunakan exception handling untuk mengenali error ini, dan terus berjalan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah kita membersihkan missing values, kita mungkin ingin melihat summary-nya. Misalnya, kita mungkin ingin melihat jumlah total missing values untuk setiap feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendapatkan jumlah missing value\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seringkali kalian harus memikirkan bagaimana kalian menangani missing values.\n",
    "\n",
    "Terkadang kalian hanya ingin menghapus baris tersebut, di lain waktu kalian menggantinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe you just want to fill in missing values with a single value\n",
    "df['ST_NUM'].fillna(125, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemungkinan besar, kita mungkin ingin melakukan imputasi berbasis lokasi/location based imputation. Inilah cara kita melakukannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2, 'ST_NUM'] = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cara yang sangat umum untuk mengganti missing values menggunakan median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df['NUM_BEDROOMS'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NUM_BEDROOMS'].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Data set tumpahan oli\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/jbrownlee/Datasets/blob/master/oil-spill.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latihan number of unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# creating the first dataframe\n",
    "df = pd.DataFrame({'A':[14,4,5,4,1],\n",
    "                   'B':[5,2,54,3,2],\n",
    "                   'C':[20,20,7,3,8],\n",
    "                   'D':[14,3,6,2,6]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gunakan fungsi dataframe.nunique() untuk menemukan nilai unik di sepanjang sumbu kolom.\n",
    "df.nunique(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A':[1,2,3],\n",
    "                   'B':[1,1,1]})\n",
    "df\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifikasi kolom yang mengandung Nilai Tunggal\n",
    "Kolom yang memiliki satu pengamatan atau nilai mungkin tidak berguna untuk pemodelan. \n",
    "\n",
    "Kolom atau prediktor ini disebut prediktor varian nol seolah-olah kita mengukur varians (nilai rata-rata dari mean), itu akan menjadi nol.\n",
    "\n",
    "Ketika sebuah prediktor berisi nilai tunggal, kami menyebutnya prediktor varian nol karena benar-benar tidak ada variasi yang ditampilkan oleh prediktor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the number of unique values for each column using numpy\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "# load the dataset\n",
    "data = loadtxt('https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv', delimiter=',')\n",
    "# summarize the number of unique values in each column \n",
    "\n",
    "print(type(data))\n",
    "\n",
    "for i in range(data.shape[1]):\n",
    "    print(i, len(unique(data[:, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv', header=None)\n",
    "# summarize the number of unique values in each column \n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of unique values for each column\n",
    "jmlunik = df.nunique()\n",
    "print(jmlunik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record columns to delete\n",
    "# for i,v in enumerate(counts) looping dulu i key , v value \n",
    "#      if v == 1  \n",
    "#          i \n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(to_del)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Columns That Conatain a Single Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv', header=None)\n",
    "print(df.shape)\n",
    "# get number of unique values for each column \n",
    "counts = df.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(to_del)\n",
    "# drop useless columns \n",
    "df.drop(to_del, axis=1, inplace=True) \n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pertimbangkan Kolom yang Memeiliki Nilia Sangat Sedikit\n",
    "\n",
    "Di bagian sebelumnya, kita melihat bahwa beberapa kolom dalam contoh dataset memiliki nilai unik yang sangat sedikit. \n",
    "\n",
    "Misalnya, ada kolom yang hanya memiliki 2, 4, dan 9 nilai unik. Ini mungkin masuk akal untuk variabel ordinal atau kategoris. Dalam hal ini, bagaimanapun, dataset hanya berisi variabel numerik. \n",
    "\n",
    "Dengan demikian, hanya memiliki 2, 4, atau 9 nilai numerik unik dalam sebuah kolom mungkin mengejutkan. \n",
    "\n",
    "Kita dapat menyebut kolom atau prediktor ini sebagai prediktor **varian mendekati nol**, karena variansnya bukan nol, tetapi *angka yang sangat kecil mendekati nol*.\n",
    "\n",
    "Tergantung pada pilihan persiapan data dan algoritma pemodelan, variabel dengan nilai numerik yang sangat sedikit juga dapat menyebabkan kesalahan atau hasil yang tidak diharapkan. \n",
    "\n",
    "Sebagai contoh, saya telah melihat mereka menyebabkan kesalahan saat menggunakan transformasi daya untuk persiapan data dan ketika memasang model linier yang mengasumsikan distribusi probabilitas data yang masuk akal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the percentage of unique values for each column using numpy\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "# load the dataset\n",
    "data = loadtxt('https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv', delimiter=',')\n",
    "# summarize the number of unique values in each column \n",
    "# data.shape --> (937,50)\n",
    "# print(data.shape[0]) # get baris \n",
    "# print(data.shape[1]) # get kolom \n",
    "for i in range(data.shape[1]): # get kolom \n",
    "    num = len(unique(data[:, i]))\n",
    "    percentage = float(num) / data.shape[0] * 100 \n",
    "    if percentage < 1:\n",
    "        print('Kurang dari 1 : %d, %d, %.1f%%' % (i, num, percentage))\n",
    "    else:\n",
    "        print('%d, %d, %.1f%%' % (i, num, percentage)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns where number of unique values is less than 1% of the rows\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv', header=None)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(counts):\n",
    "    if (float(v)/df.shape[0]*100) < 1:\n",
    "        print(' : %d, %d, %.1f%%' % (i, v, float(v)/df.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < 1] \n",
    "print(to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "#drop useless columns\n",
    "#df.drop(to_del, axis=1, inplace=True)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify ROws That Conatain DUplicate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di sini, baris duplikat adalah baris di mana setiap nilai di setiap kolom untuk baris itu muncul dalam urutan yang sama (nilai kolom yang sama) di baris lain.\n",
    "\n",
    "... jika Anda telah menggunakan data mentah yang mungkin memiliki entri duplikat, menghapus data duplikat akan menjadi langkah penting untuk memastikan data Anda dapat digunakan secara akurat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate rows of duplicate data\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv', header=None) # calculate duplicates\n",
    "dups = df.duplicated()\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report if there are any duplicates \n",
    "print(dups.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all duplicate rows \n",
    "print(df[dups])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete duplicate rows \n",
    "#df.drop_duplicates(inplace=True) \n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Mark and Removing Missing Value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data dunia nyata sering kali memiliki nilai yang hilang. Data dapat memiliki nilai yang hilang karena sejumlah alasan seperti pengamatan yang tidak dicatat dan kerusakan data. \n",
    "\n",
    "Menangani data yang hilang itu penting karena banyak algoritme pembelajaran mesin tidak mendukung data dengan nilai yang hilang. Dalam hal ini kita akan menemukan cara menangani data yang hilang untuk pembelajaran mesin dengan Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cara menandai nilai yang tidak valid atau rusak sebagai hilang dalam kumpulan data Anda.\n",
    "* Bagaimana mengkonfirmasi bahwa kehadiran nilai hilang yang ditandai menyebabkan masalah untuk algoritma Machine Learning.\n",
    "* Cara menghapus baris dengan data yang hilang dari kumpulan data Anda dan mengevaluasi algoritme pembelajaran pada kumpulan data yang diubah."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes Dataset\n",
    "\n",
    "Dataset mengklasifikasikan data pasien baik sebagai onset diabetes dalam waktu lima tahun atau tidak. Ada 768 contoh dan delapan variabel input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "dataset = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv', header=None) # summarize the dataset\n",
    "dataset.head()\n",
    "#dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tandai Nilai yang Hilang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebagian besar data memiliki nilai yang hilang, dan kemungkinan nilai yang hilang meningkat seiring dengan ukuran kumpulan data.\n",
    "Data yang hilang tidak jarang dalam kumpulan data nyata. Faktanya, peluang bahwa setidaknya satu titik data hilang meningkat seiring dengan bertambahnya ukuran kumpulan data.\n",
    "\n",
    "\n",
    "Di bagian ini, kita akan melihat bagaimana kita dapat mengidentifikasi dan menandai nilai sebagai hilang. Kita dapat menggunakan plot dan ringkasan statistik untuk membantu mengidentifikasi data yang hilang atau rusak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat melihat bahwa ada kolom yang memiliki nilai minimal nol (0). Pada beberapa kolom, nilai nol tidak masuk akal dan menunjukkan nilai yang tidak valid atau hilang.\n",
    "Nilai yang hilang sering ditunjukkan oleh entri di luar rentang; mungkin angka negatif (misalnya, -1) di bidang numerik yang biasanya hanya positif, atau 0 di bidang numerik yang biasanya tidak pernah menjadi 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the following columns have an invalid zero minimum value:\n",
    "* 1: Plasma glucose concentration \n",
    "* 2: Diastolic blood pressure\n",
    "* 3: Triceps skinfold thickness\n",
    "* 4: 2-Hour serum insulin\n",
    "* 5: Body mass index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head(20))\n",
    "#kita lihat 0 values pada columns 2,3,4 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita bisa mendapatkan hitungan jumlah nilai yang hilang pada masing-masing kolom ini. Kita dapat melakukan ini dengan menandai semua nilai dalam subset DataFrame yang kita minati yang memiliki nilai nol sebagai True. kita kemudian dapat menghitung jumlah nilai sebenarnya di setiap kolom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing = (dataset[[1,2,3,4,5]] == 0).sum()\n",
    "# report the results\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan) # count the number of nan values in each column print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the first 20 rows of data \n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows With Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the shape of the data with missing rows removed \n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example where missing values cause errors\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# load the dataset\n",
    "dataset = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv', header=None) # summarize the dataset\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "# define the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# # define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "# # evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy') \n",
    "# # report the mean performance\n",
    "print('Accuracy: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "        \n",
    "Error caused by the presence of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ini seperti yang kita harapkan. Kami dicegah untuk mengevaluasi algoritme LDA (dan algoritme lainnya) pada kumpulan data dengan nilai yang hilang.\n",
    "Banyak model prediktif populer seperti vector machines, the glmnet, and neural networks, tidak dapat mentolerir sejumlah nilai yang hilang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Rows With Missing Values\n",
    "\n",
    "\n",
    "Strategi paling sederhana untuk menangani data yang hilang adalah dengan menghapus catatan yang berisi nilai yang hilang.\n",
    "Pendekatan paling sederhana untuk menangani nilai yang hilang adalah dengan menghapus seluruh prediktor dan/atau sampel yang mengandung nilai yang hilang.\n",
    "\n",
    "Kita dapat melakukan ini dengan membuat DataFrame Pandas baru dengan menghapus baris yang berisi nilai yang hilang. \n",
    "\n",
    "Pandas menyediakan fungsi dropna() yang dapat digunakan untuk menjatuhkan kolom atau baris dengan data yang hilang. Kita dapat menggunakan dropna() untuk menghapus semua baris dengan data yang hilang, sebagai berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example where missing values cause errors\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# load the dataset\n",
    "dataset = read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv', header=None) # summarize the dataset\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "# drop rows with missing values\n",
    "dataset.dropna(inplace=True)\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "# define the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# # define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "# # evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy') \n",
    "# # report the mean performance\n",
    "print('Accuracy: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Obesity in England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xlrd>=1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "data = pd.ExcelFile('obes.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita akan fokus pada sheet 7.2, sekarang jika melihat table 7.2 pada excel, kita akan melihat bahwa 4 baris teratas dan 14 baris terbawah berisi info yang tidak berguna, kita hanya membutuhkan baris 5-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_age = data.parse(u'7.2', skiprows=4, skipfooter=14)\n",
    "data_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baris pertama mewakili columns headers. Kita memiliki baris kosong dile asli, dan itu, muncul sebagai NaN (Bukan angka)\n",
    "\n",
    "Jadi sekrang kita perlu melakukan 2 hal:\n",
    "- Rename the first headr to Year, and\n",
    "- Get rid of any empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengganti nama kolom Unnamed: 0 menjadi year\n",
    "data_age.rename(columns={u'Unnamed: 0':u'Year'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghapus baris kosong yang diisi dengan NaN\n",
    "data_age.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika kita melihat data_age, nilai pertama adalah angka. Ini adalah indeksnya, dan pandas menggunakan paraktik excel default yang memiliki angka sebagia indeks. Kita akan mengubah indeks menjadi Tahun. ini akan mempermudak pembuatan plot, karena indeks biasanya diplot sebagai sumbu x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_age.set_index('Year', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_age.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di lihat dari grafik di atas, jika di lihat ada yang salah yaitu data asli kita berisi total yang menutupi bidang lainnya. Kita perlu mentingkitkannya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus kolom Total dari datase\n",
    "data_age_minus_total = data_age.drop('Total', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ulang\n",
    "data_age_minus_total.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jauh lebih baik dari sbelumnya, kita benar-benar dapat melihat kelompok individu sekrang. Bisakah kita melihat kelompok usia mana yang memiliki obesitas tertinggi?\n",
    "\n",
    "Kembali ke peryantaan awal kita: Are children getting fatter?\n",
    "\n",
    "Mari plot sebagian kecil data: anak-anak dibawah usia 16 tahun dan orang dewasa dengan usia 35-44 tahun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_age['Under 16'].plot(label = 'Under 16', legend = True)\n",
    "data_age['35-44'].plot(label = '35-44', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari grafik diatas dapat disimpulkan, ketika obesitas anak sedikit mennurun, orang tua terus membengkak. Jadi nampkanya para orang tua harus menghawatirkan diri sendiri daripada menghawatirkan anaknya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series\n",
    "### Basic Time Series Manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rng = pd.date_range(start='1/01/2020', end='1/08/2020', freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita buat contoh data frame dengan timestamp dan lihat 15 elemen pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_rng, columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'] = np.random.randint(0,100,size=(len(data_rng)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika kita ingin melakukan manipulasi deret waktu/time series, kita perlu memiliki date time index sehingga data frame kita di indeks pada timestamp\n",
    "\n",
    "Konversikan indeks data frame menjadi datetime index dan tampilkan elemen pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('datetime')\n",
    "df.drop(['date'], axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagaimana jika 'time' stamps dalam data kita sebernarnya berjenid string vs numberik?\n",
    "\n",
    "Mari kita ubah data_rng kita menjadi list of string dan kemudian ubah string tersebut menjadi stempel waktu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data_rng = [str(x) for x in data_rng]\n",
    "string_data_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat mengkonversi string menjadi timestamps dengan melihat formatnya, lalu melihat nilainya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_data_rng = pd.to_datetime(string_data_rng,\n",
    "                                    infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_data_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tetapi bagaimana jika perlu mengubah format unique string\n",
    "\n",
    "Mari kita buat daftar tunggal yang berubah-ibah menjadi string dan mengubahnya menjadi timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data_rng_2 = ['June-01-2020', 'June-02-2020', 'June-03-2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_data_rng_2 = [datetime.strptime(x, '%B-%d-%Y') for x in string_data_rng_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_data_rng_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperti apa jadinya jika kita memasukan ini ke dalam data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(timestamp_data_rng_2, columns=['date'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kembali ke data frame asli kita, mari kita lihat datanya dengan parsing pada timestamp index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katakanlah kita hanya ingin melihat data di mana tanggalnya adalah tanggal 2 setiap bulannya, kita bisa menggunakan indeks seperti dibawah ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index.day == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga bisa langsung memanggil tanggal yang ingin kita lihat memaui index dari data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2020-01-03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagaimana dengan memilih data di antara tanggal tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2020-01-04': '2020-01-06']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data frame yang telah kita buat memberi kita data dengan frekuensi per jam, tetapi kita dapat mengambil ulang sampel/resample data pada frekuensi yang berbeda dan menentukan bagaimana menentukan bagaimana kita ingin menghitung summary statistic untuk frekuensi sample baru\n",
    "\n",
    "kita dapat mengambil min, max, average,sum, dll, dari data frekuensi harian dari pada frekuensi per jam seperti contoh di bawah ini tempat kita menghitung rata-rata harian dari data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample('D').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagaimana dengan window statistics seperti rolling mean atau rolling sum?\n",
    "\n",
    "Mari buat kolom baru di df asli kota yang menghitung rolling sum selama periode 3 window dan kemudian lihat di bagina atas data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rolling_sum'] = df.rolling(3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat melihat bahwa pandas menghitung dengan benar dan hanya memiliki nilai yang valid ketika ada tiga periode untuk melihat ke belakang\n",
    "\n",
    "Ini adalah kesempatan bagus untuk melihat bagaimana kita dapat melakukan forwarding of backfilling data saar bekerja dengan nilai data yang hilang\n",
    "\n",
    "Brikut df kita, tetapi dengan kolom bari yang mengabil rolling sum dan backfills data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rolling_sum_backfilled'] = df['rolling_sum'].fillna(method = 'backfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily = pd.read_csv('https://raw.githubusercontent.com/ardhiraka/PFDS_sources/master/opsd_germany_daily.csv')\n",
    "opsd_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kolom Date adalah tipe data yang benar, mari kita set sebagai indeks dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily = opsd_daily.set_index('Date')\n",
    "opsd_daily.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatifnya, kita dapat menggabungkan langkah-langkah di atas menjadi satu baris, menggunakan parameter index_col dan parse_dates dari pungsi read.csv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily = pd.read_csv('https://raw.githubusercontent.com/ardhiraka/PFDS_sources/master/opsd_germany_daily.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspek berguna lainnya dari DatetimeIndex adalah bahwa setiap komponen tanggal / waktu tersedia sebagai atribut seperti tahun, bulan, hari dan setrusnya.Mari tambahkan beberapa kolom lagi ke opsd_daily yang berisi tahun, bulan dan hari kerja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily['Year'] = opsd_daily.index.year\n",
    "opsd_daily['Month'] = opsd_daily.index.month\n",
    "opsd_daily['Weekday'] = opsd_daily.index.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat memilih data untuk satu hari menggunakan string seperti '2017-08-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.loc['2017-08-10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita juga dapat memilih slice hari, seperti '2014-01-20':'2014-01-22'. Seperti pengindeksan berbasis label biasa dengan loc, slice tersebut menyertakan kedua titik akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.loc['2014-01-20':'2014-01-22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitur lain yang sangat berguna dari time series pandas adalah partial-string indexing, dimana kita dapat memilih semua tanggal / waktu yang sebagian cocok denagan string yang di berikan , misalnya kita dapat memilih seluruh tahun 2006 dengan opsd_daily.loc['2006']\n",
    ", atau seluruh selurh bulan februari 2012 dengan opsd_daily.loc['2012-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.loc['2012-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita buat ine plot dari time series komsumsi listrik harian di jerman, menggunakan plot() dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily['Consumption'].plot(linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat melihat bahwa method plot() memilih lokasi tick yang cukup bagus(setiap dua tahun) dan label(years) untuksumbu x, yang sangat membantu. Namun, dengan banyaknya tiitk data, line plot menjadi padat dan sulit untuk dibaca "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita plot data sebagian titik-titik dan lihat juga time series dan matahari dan angin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily['Consumption'].plot(marker='.', alpha=0.5, linestyle='None',\n",
    "                              figsize=(11,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily['Solar'].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily['Wind'].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita sudah bisa melihat beberapa pola menarik yang muncul:\n",
    "\n",
    "- Konsumsi listrik tertinggi ada di musim dingin, kemungkinan karena pemanas listrik dan peningkatan penggunaan penerangan, dan terendah di musim panas.\n",
    "\n",
    "- Produksi tenaga surya paling tinggi di musim panas, saat sinar matahari paling melimpah, dan paling rendah di musim dingin.\n",
    "\n",
    "- Produksi tenaga angin paling tinggi di musim dingin, mungkin karena angin yang lebih kuat dan badai yang lebih sering, dan paling rendah di musim panas.\n",
    "\n",
    "- Tampaknya ada tren peningkatan yang kuat dalam produksi tenaga angin selama bertahun-tahun.\n",
    "\n",
    "Ketiga time series dengan jelas menunjukkan periodisitas — sering disebut sebagai seasonality dalam time series analysis — di mana sebuah pola berulang berulang kali pada interval waktu yang teratur. \n",
    "\n",
    "Rangkaian waktu Consumption, Solar, dan Wind berosilasi antara nilai tinggi dan rendah dalam skala waktu tahunan, sesuai dengan perubahan cuaca musiman sepanjang tahun. \n",
    "\n",
    "Namun, musim secara umum tidak harus sesuai dengan musim meteorologi. Misalnya, data penjualan ritel sering kali menunjukkan musim tahunan dengan peningkatan penjualan pada bulan November dan Desember, menjelang liburan.\n",
    "Seasonality juga dapat terjadi pada skala waktu lain. Plot di atas menunjukkan bahwa konsumsi listrik Jerman mungkin terlihat weekly seasonality, sesuai dengan hari kerja dan akhir pekan. \n",
    "\n",
    "Mari kita plot deret waktu dalam satu tahun untuk menyelidiki lebih lanjut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = opsd_daily.loc['2017', 'Consumption'].plot(figsize=(11,9))\n",
    "ax.set_ylabel('Daily Consumption(GWH)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kita dapat dengan jelas melihat osilasi mingguan/weekly oscillations. Ciri menarik lain yang terlihat pada tingkat granularitas ini adalah penurunan drastis konsumsi listrik pada awal Januari dan akhir Desember, selama liburan.\n",
    "\n",
    "Mari kita perbesar lebih jauh dan lihat bulan Januari dan Februari saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = opsd_daily.loc['2017-01':'2017-02', 'Consumption'].plot(marker='o',\n",
    "                                                             linestyle='-', figsize=(11,9))\n",
    "ax.set_ylabel('Daily COnsumption (GWH)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, mari kita jelajahi lebih jauh seasonality data kita dengan fungsi box plots untuk mengelompokkan data berdasarkan periode waktu yang berbeda dan menampilkan distribusi untuk setiap kelompok. Pertama-tama, kita akan mengelompokkan data berdasarkan bulan, untuk memvisualisasikan yearly seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.boxplot(column=['Consumption'], by='Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.boxplot(column=['Wind'], by='Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.boxplot(column=['Solar'], by='Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "box plots berikut mengonfirmasi yearly seasonality yang kitalihat di plot sebelumnya dan memberikan beberapa insights:\n",
    "\n",
    "Meskipun konsumsi listrik umumnya lebih tinggi di musim dingin dan lebih rendah di musim panas, \n",
    "\n",
    "median dan dua kuartil lebih rendah pada bulan Desember dan Januari dibandingkan dengan November dan Februari, kemungkinan karena bisnis tutup selama liburan. \n",
    "\n",
    "Kita melihat ini dalam rangkaian waktu untuk tahun 2017, dan box plot menegaskan bahwa ini adalah pola yang konsisten selama bertahun-tahun.\n",
    "\n",
    "Sementara produksi tenaga surya dan angin sama-sama menunjukkan yearly seasonality, distribusi tenaga angin memiliki lebih banyak outliers, yang mencerminkan efek kecepatan angin ekstrem sesekali yang terkait dengan badai dan kondisi cuaca sementara lainnya.\n",
    "\n",
    "Selanjutnya, mari kelompokkan rangkaian waktu konsumsi listrik berdasarkan hari dalam seminggu, untuk menjelajahi weekly seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsd_daily.boxplot(column=['Consumption'], by='Weekday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti yang diharapkan, konsumsi listrik secara signifikan lebih tinggi pada hari kerja dibandingkan pada akhir pekan. Outliers rendah pada hari kerja mungkin selama hari libur/holidays.\n",
    "\n",
    "Seringkali berguna untuk resample data time series kita ke frekuensi yang lebih rendah atau lebih tinggi. \n",
    "\n",
    "Resampling ke frekuensi yang lebih rendah (downsampling) biasanya melibatkan operasi agregasi - misalnya, menghitung total penjualan bulanan dari data harian. \n",
    "\n",
    "Resampling ke frekuensi yang lebih tinggi (upsampling) kurang umum dan sering kali melibatkan interpolasi atau metode pengisian data lainnya - misalnya, menginterpolasi data cuaca setiap jam hingga interval 10 menit untuk dimasukkan ke model ilmiah.\n",
    "\n",
    "Kita akan fokus di sini pada downsampling, mengeksplorasi bagaimana hal itu dapat membantu kita menganalisis data OPSD dalam berbagai skala waktu. \n",
    "\n",
    "Kita menggunakan method resample() DataFrame, yang membagi DatetimeIndex ke dalam time bins dan mengelompokkan data menurut time bin. \n",
    "\n",
    "Metode resample() mengembalikan objek Resampler, mirip dengan objek GroupBy pandas. \n",
    "\n",
    "Kita kemudian dapat menerapkan metode agregasi seperti  mean(), median(), sum(), dll., Ke grup data untuk setiap time bin.\n",
    "\n",
    "Misalnya, mari kita resample data menjadi weekly mean time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['Consumption','Wind','Solar','Wind+Solar']\n",
    "opsd_weekly_mean = opsd_daily[data_columns].resample('W').mean()\n",
    "opsd_weekly_mean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baris pertama di atas, berlabel 2006-01-01, berisi rata-rata dari semua data yang ada dalam time bin 2006-01-01 hingga 2006-01-07. Baris kedua, berlabel 2006-01-08, berisi data rata-rata untuk time bin 2006-01-08 hingga 2006-01-14, dan seterusnya.\n",
    "\n",
    "Berdasarkan konstruksi, deret waktu mingguan kita memiliki 1/7 poin data sebanyak deret waktu harian. Kita dapat mengonfirmasi ini dengan membandingkan jumlah baris dari dua DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opsd_daily.shape[0])\n",
    "print(opsd_weekly_mean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita ganbarkan rangkaianwaktu solar harian dan mingguan dalam satu periode enam bulan untuk membandingkannya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = '2017-01', '2017-06'\n",
    "\n",
    "opsd_daily.loc[start:end, 'Solar'].plot(marker='.', linestyle='-',\n",
    "                                        linewidth=0.5, figsize=(11,9))\n",
    "opsd_weekly_mean.loc[start:end, 'Solar'].plot(marker='o', markersize=8,\n",
    "                                              linestyle='-', figsize=(11,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat melihat bahwa deret waktu rata-rata mingguan lebih halus daripada deret waktu harian karena variabilitas frekuensi yang lebih tinggi telah dirata-ratakan dalam resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = '2017-01', '2017-06'\n",
    "\n",
    "opsd_daily.loc[start:end, 'Wind'].plot(marker='.', linestyle='-',\n",
    "                                        linewidth=0.5, figsize=(11,9))\n",
    "opsd_weekly_mean.loc[start:end, 'Wind'].plot(marker='o', markersize=8,\n",
    "                                              linestyle='-', figsize=(11,9))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa45304f70ae8036a3612989fe56b812936e25471e0e2293779c048f0363d872"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
